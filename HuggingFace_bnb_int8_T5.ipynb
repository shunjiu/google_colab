{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shunjiu/google_colab/blob/main/HuggingFace_bnb_int8_T5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace meets `bitsandbytes` for lighter models on GPU for inference\n",
        "\n",
        "## Running T5-11b on Google Colab "
      ],
      "metadata": {
        "id": "Gbr9iYfRy4GZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <center>\n",
        " <img src=\"https://s3.amazonaws.com/moonup/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png\">\n",
        " </center>"
      ],
      "metadata": {
        "id": "32ShdTnhMoKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You can run your own 8-bit model on any HuggingFace ðŸ¤— model with just few lines of code. This notebook shows how to do it with a `T5` model that would usually require 12GB of GPU RAM.\n",
        "Install the dependencies below first!\n"
      ],
      "metadata": {
        "id": "K9uJn2NczFBK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aep1KMF6dqdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ddcc2d-245e-469c-bc9f-b45999ad4dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet bitsandbytes\n",
        "!pip install --quiet --upgrade transformers # Install latest version of transformers\n",
        "!pip install --quiet --upgrade accelerate\n",
        "!pip install --quiet sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers bitsandbytes accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6YTOisTFDsR",
        "outputId": "101024e6-60bd-4b7a-8aaa-ee1b8331c78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.8/dist-packages (0.35.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.8/dist-packages (0.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.13.0+cu116)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose your model"
      ],
      "metadata": {
        "id": "ymZwkJUenuV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rerun this cell if you want to change the model!"
      ],
      "metadata": {
        "id": "_3cMSVC-oCyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"t5-3b-sharded\" #@param [\"t5-11b-sharded\", \"t5-3b-sharded\"]"
      ],
      "metadata": {
        "id": "3HibeFxJnwq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use 8bit models with `t5-3b-sharded` ðŸ¤—"
      ],
      "metadata": {
        "id": "BejbQStU5Eik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# T5-3b and T5-11B are supported!\n",
        "# We need sharded weights otherwise we get CPU OOM errors\n",
        "model_id=f\"ybelkada/{model_name}\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model_8bit = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\", load_in_8bit=True)"
      ],
      "metadata": {
        "id": "YJlldexxwnhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the memory footprint of this model! ðŸª¶"
      ],
      "metadata": {
        "id": "OtvnO-jtM1is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_8bit.get_memory_footprint()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDgDNAmYM1Lx",
        "outputId": "3947d017-06b9-44fa-bb90-bab4db5d5cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2884624384"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `t5-3b` the int8 model is about ~2.9GB! whereas the original model has 11GB. For `t5-11b` the int8 model is about ~11GB vs 42GB for the original model.\n",
        "Now let's generate and see the qualitative results of the 8bit model!"
      ],
      "metadata": {
        "id": "TsuYzVoBPMtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 50\n",
        "\n",
        "input_ids = tokenizer(\n",
        "    \"translate English to German: Hello my name is Younes and I am a Machine Learning Engineer at Hugging Face\", return_tensors=\"pt\"\n",
        ").input_ids  \n",
        "\n",
        "outputs = model_8bit.generate(input_ids, max_new_tokens=max_new_tokens)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "j1s0spY4icGK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}